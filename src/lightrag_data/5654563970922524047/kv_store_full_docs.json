{
  "doc-4663038186c0d2b2f2e2d44ef682a821": {
    "content": "Deep neural network autoencoders are routinely used computationally for model\nreduction. They allow recognizing the intrinsic dimension of data that lie in a\n$k$-dimensional subset $K$ of an input Euclidean space $\\mathbb{R}^n$. The\nunderlying idea is to obtain both an encoding layer that maps $\\mathbb{R}^n$\ninto $\\mathbb{R}^k$ (called the bottleneck layer or the space of latent\nvariables) and a decoding layer that maps $\\mathbb{R}^k$ back into\n$\\mathbb{R}^n$, in such a way that the input data from the set $K$ is recovered\nwhen composing the two maps. This is achieved by adjusting parameters (weights)\nin the network to minimize the discrepancy between the input and the\nreconstructed output. Since neural networks (with continuous activation\nfunctions) compute continuous maps, the existence of a network that achieves\nperfect reconstruction would imply that $K$ is homeomorphic to a\n$k$-dimensional subset of $\\mathbb{R}^k$, so clearly there are topological\nobstructions to finding such a network. On the other hand, in practice the\ntechnique is found to \"work\" well, which leads one to ask if there is a way to\nexplain this effectiveness. We show that, up to small errors, indeed the method\nis guaranteed to work. This is done by appealing to certain facts from\ndifferential topology. A computational example is also included to illustrate\nthe ideas."
  }
}