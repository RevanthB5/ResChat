{
  "chunk-fdfce24a4a26f0d1ba1c968c854375b8": {
    "tokens": 1200,
    "content": "Structuring Autoencoders\nMarco Rudolph\nBastian Wandt\nBodo Rosenhahn\nLeibniz Universit¨at Hannover\n{rudolph, wandt, rosenhahn}@tnt.uni-hannover.de\nAbstract\nIn this paper we propose Structuring AutoEncoders\n(SAE). SAEs are neural networks which learn a low dimen-\nsional representation of data and are additionally enriched\nwith a desired structure in this low dimensional space.\nWhile traditional Autoencoders have proven to structure\ndata naturally they fail to discover semantic structure that is\nhard to recognize in the raw data. The SAE solves the prob-\nlem by enhancing a traditional Autoencoder using weak su-\npervision to form a structured latent space.\nIn the experiments we demonstrate, that the structured\nlatent space allows for a much more efﬁcient data represen-\ntation for further tasks such as classiﬁcation for sparsely\nlabeled data, an efﬁcient choice of data to label, and morph-\ning between classes. To demonstrate the general applicabil-\nity of our method, we show experiments on the benchmark\nimage datasets MNIST, Fashion-MNIST, DeepFashion2 and\non a dataset of 3D human shapes.\n1. Introduction and Related Work\nData structuring is widely used to analyze, visualize and\ninterpret information. A common approach is to employ\nautoencoders [11] which try to solve this task by structur-\ning data in an unsupervised fashion. Unfortunately, they\ntend to focus on the most dominant structures in the data\nwhich not necessarily incorporate meaningful semantics.\nIn this paper we propose Structuring AutoEncoders (SAE)\nwhich enhance traditional autoencoders with weak supervi-\nsion. These SAEs can enforce a structure in the latent space\ndesired by a user and are able to separate the data accord-\ning to even subtle differences. The structured latent space\nopens up a variety of applications:\n1. Improving classiﬁcation accuracy on datasets where\nonly a small number of data points is labeled.\n2. Finding the most important unlabeled data points for\ngiving labeling recommendations.\n3. An interpretable latent space for data visualization.\nFigure 1. Latent spaces of the autoencoders for the 3D HumanPose\ndatabase. The colors are given by the gender, male and female.\nLeft: Confused latent space when using a traditional autoencoder.\nRight: Clustered structure in latent space when using the SAE.\n4. Morphing between properties that are hidden in the\ndata.\nThe focus of this work is to transfer data into an orga-\nnized structure that reﬂects a meaningful representation. To\nachieve this, it is necessary to uncover even subtle semantic\ncharacteristics of data. As an enhancement of linear fac-\ntorization models [9], the idea of autoencoders as a tool to\nnaturally uncover structures has been part of research on\nneural networks for decades [15, 3, 29]. They are com-\nmonly used to learn representative data codings and usu-\nally consist of a neural network having an encoder and a\ndecoder. The encoder maps the data points through one\nor more hidden layers to a low dimensional latent space\nfrom where the decoder reconstructs the input. However,\nthis representation is not necessarily meaningful in terms\nof the underlying semantics and cannot discover well hid-\nden structures. There are other variants of Autoencoders\nwhich enforce a speciﬁc distribution in the latent space, ei-\nther by a variational approach [12] or by applying a dis-\ncriminator network on the latent space known as Adver-\nsarial Autoencoders [20].\nOther works focussed on get-\nting disentangled representations of data in the latent space\n[14, 7, 10, 1]. There are several other variants that ﬁnd ad-\nditional constraints on the latent variables, mostly for spe-\nciﬁc applications [22, 6, 25, 18, 4, 17, 5]. However, analy-\nsis of hidden structures is rarely considered. Our approach\nsolves this task by improving traditional autoencoders with\na weak supervision using only a very small amount of addi-\n1\narXiv:1908.02626v1  [cs.LG]  7 Aug 2019\ntionally labeled data which represents the desired formerly\nwell-hidden semantics. Furthermore, we propose a method\nto extend this small set of labels efﬁciently by determining\ncritical examples that are most meaningful to improve clas-\nsiﬁcation. Comparing common classiﬁcation networks to\nour approach, they can be interpreted as the omission of the\ndecoder network.\nAs an example we consider the separation of male and\nfemale 3D body shapes which are in different poses. The\nobvious structure in the data is the pose of the body shapes\nsince the variation in pose is a lot stronger compared to vari-\nation in the gender regarding the reconstruction error. In\nfact, passing the data through a traditional autoencoder it\nwill mix male and female data points as can be seen on the\nleft hand side of Fig. 1. To assist the autoencoder to separate\nthe data points into male and female we deﬁne distances be-\ntween different classes. These distances shall be maintained\nin the latent space while training the SAE. Following the ex-\nample we specify a distance of 1 between the male and fe-\nmale class. The distance metric is freely customizable to a\ndesired task. The right image of Fig. 1 shows a much better\norganized latent space obtained by the SAE. Interestingly,\nthere is only a marginal increase of the reconstruction error\nwhen using the SAE compared to standard autoencoders.\nFor ordering",
    "chunk_order_index": 0,
    "full_doc_id": "doc-f06c2ff9664d461ea472bbf88f2a9a4f"
  },
  "chunk-8540801279f57be4b29900eb965bc9b7": {
    "tokens": 1200,
    "content": "tween different classes. These distances shall be maintained\nin the latent space while training the SAE. Following the ex-\nample we specify a distance of 1 between the male and fe-\nmale class. The distance metric is freely customizable to a\ndesired task. The right image of Fig. 1 shows a much better\norganized latent space obtained by the SAE. Interestingly,\nthere is only a marginal increase of the reconstruction error\nwhen using the SAE compared to standard autoencoders.\nFor ordering data with respect to the relative distance mea-\nsures in this work Multidimensional Scaling (MDS) is ap-\nplied [33]. Alternative approaches such as t-SNE, which\nis based on a Stochastic Neighbor Embedding [27, 26] or\nUniform Manifold Approximation and Projection (UMAP)\n[21] are conceivable. These methods can be used to visual-\nize the level of similarity of individual examples of a dataset\nand can be seen as related ordination techniques which is\nused in information visualization. To preserve desired dis-\ntances in the latent space we use MDS in this work. By\napplying MDS on sparsely known labels of the training set,\nit allows to structure the data in such a fashion, that data\npoints with the same labels have a small distance in the la-\ntent space, whereas data belonging to different labels are\nenforced to keep a certain distance. This is formulated as\nthe structural loss in addition to the decoders reconstruction\nerror. A diagram of the proposed autoencoder training in-\ncluding a structured latent space visualization and the used\nlosses is shown in Fig. 2.\nWe show experiments on the benchmark dataset MNIST\n[16] which we randomly decompose into three classes. The\nresults underline the fact that the SAE efﬁciently separates\nthe latent space according to a freely selected structure that\nis invisible the raw data. Moreover, using only a very sparse\nset of data (6000 labeled samples) the SAE outperforms\ncomparable neural networks trained solely for the classiﬁ-\ncation task. These results are conﬁrmed on the recent more\ndiverse dataset Fashion-MNIST [31] and our own dataset\nof 3D meshes of human body shapes. A real-world applica-\nFigure 2. Our Structuring AutoEncoder (SAE) projects data into a\nstructured latent space. It uses Multidimensional Scaling to calcu-\nlate the class centers in the latent space. By applying an additional\nstructural loss the SAE maintains distances between the classes\naccording to a desired metric. Losses are colored in blue.\ntion is shown on the recently published DeepFashion2 [8]\ndataset where our SAE outperforms comparable classiﬁers.\nAdditionally, we show that our guided labeling approach\nonly needs 600 training samples combined with the 100\nmost meaningful samples that are automatically detected to\nachieve good classiﬁcation results. This provides a tool to\nsigniﬁcantly reduce labeling time and cost.\nSummarizing, our contributions are:\n• An autoencoder that structures data according to given\nclasses and preserves distances present in the label\nspace.\n• A method to deal with sparsely labeled data while pre-\nventing the overﬁtting of traditional approaches.\n• Better classiﬁcation performance than comparable\nneural networks trained for classiﬁcation using the\nsame amount of training data.\n• Similar training performance (reconstruction loss)\nwith and without structured training.\n• A technique to improve the labeling efﬁciency by de-\ntermining critical data points.\n2. Structuring Autoencoder\nWe assume that the input data can be separated into sev-\neral classes which are not obvious in the data itself. These\nclasses are only known for a small fraction of the input data.\nWe further assume that the data can be projected to a latent\nspace that preserves the distances between the classes. As\na toy example we separate the Fashion-MNIST dataset [31]\ninto the three classes summer clothes (top, sandals, dress\nand shirt), winter clothes (pullover, coat and ankle boot),\nand all-year fashion (sneaker, trousers, bag). The left hand\nside of Fig. 5 shows the latent space of this. Here, as an ex-\nample we deﬁne an equal distance between the classes. Ob-\nviously, the season depending decomposition is not given\nby the data itself. The following sections describe the pro-\nposed autoencoder architecture and training. Algorithm 1\ndescribes the steps for training the network.\nAlgorithm 1 Autoencoder training\nX ←training samples\nD ←distances\nwhile no convergence do\nZ = fenc(X) {project X into latent space}\nZ∗= MDS(Z) {calculate desired positions}\nZZ+ = USV T {singular value decomposition}\nset all singular values ≥0 to 1\nR = US∗V T {calculate ideal rotation}\n˜Z = RZ∗{ﬁnal positions in latent space}\ntrain SAE with loss LSAE(x, ˜z) and LAE(x, fae(x))\nend while\n2.1. Architecture and Loss Functions\nOur method is not restricted to a speciﬁc autoencoder ar-\nchitecture. That means every architecture can be applied,\nfor instance fully connected, (fully) convolutional, or ad-\nversarial autoencoders. We deﬁne two loss functions. The\nﬁrst loss\nLAE(x, fae(x)) = ∥x −fae(x)∥2\n2,\n(1)\nis the mean squared error (MSE) between the input x and\nthe output of the autoencoder fae(x) as it is commonly\nused.\nWith fenc(x) as the",
    "chunk_order_index": 1,
    "full_doc_id": "doc-f06c2ff9664d461ea472bbf88f2a9a4f"
  },
  "chunk-e09194c38e45e88e7b2b1adf9be5a7c6": {
    "tokens": 1200,
    "content": ",\nfor instance fully connected, (fully) convolutional, or ad-\nversarial autoencoders. We deﬁne two loss functions. The\nﬁrst loss\nLAE(x, fae(x)) = ∥x −fae(x)∥2\n2,\n(1)\nis the mean squared error (MSE) between the input x and\nthe output of the autoencoder fae(x) as it is commonly\nused.\nWith fenc(x) as the function of the encoder that\nprojects x to the latent space a structural loss is deﬁned as\nLS(fenc(x), ˜z) = ∥fenc(x) −˜z∥2\n2.\n(2)\nIt is calculated by the MSE between the latent values\nfenc(x) and the desired locations ˜z in the latent space that\nare calculated at each iteration. The estimation of these lo-\ncations using Multidimensional Scaling is described later in\nSec. 2.3. This gives the combined loss\nLSAE(x, ˜z) =\nγLS(fenc(x), ˜z) + (1 −γ)LAE(x, fae(x)),\n(3)\nwith γ = [0, 1] as the balancing parameter between the two\nlosses. Note that γ = 0 corresponds to the traditional au-\ntoencoder training while a higher value of γ gives a higher\nimportance to the structural loss. In section 3.6 the inﬂuence\nof γ is analyzed and its choice for experiments is explained.\nFor unlabeled data LSAE = LAE is considered since there\nis no ˜z deﬁned.\n2.2. Initialization\nFollowing the toy example from above a distance matrix\nD between the three classes is calculated where each row\nand column marks a training sample and the entries are the\ndistances. Here, we can deﬁne an equal distance (e.g. of\n1) between different classes. The intra class distance is 0.\nSince the distances between the classes stay the same dur-\ning training the distance matrix only needs to be calculated\nonce.\n2.3. Structuring the latent space\nThe autoencoder is trained iteratively. In every iteration\nthe data x is projected into the latent space by the encoder\nwhich gives the latent variables\nz = fenc(x).\n(4)\nThis is done for the complete training set. By stacking all z\nvectors we obtain the matrix Z. To calculate the desired la-\ntent positions ˜Z we apply Multidimensional Scaling (MDS)\n[13] to the distance matrix D that is deﬁned in Section 2.2.\nMDS is able to arrange data points in a space of an arbi-\ntrary dimension in a way that the given distances should be\npreserved. The Shepard-Kruskal algorithm [13] is an iter-\native method to ﬁnd such an arrangement. After an initial-\nization the stress between the actual and the given distance\nmeasures is minimized until a local minimum is found. In\ncontrast to manually setting the desired latent locations the\nMDS can automatically adapt to the data and therefore to\nthe training process. This results in a target matrix of loca-\ntions Z∗in the latent space.\nSince there is an inﬁnite number of possible target loca-\ntions and we want to compute locations close to Z the MDS\nalgorithm is initialized with them. To get the best possible\ntarget locations an orthogonal alignment [23] is applied to\nZ∗to best ﬁt Z. Naturally, MDS results in centralized data\npoints. Therefore, we only need to compute the ideal rota-\ntion around the origin. Let P be a projection matrix that\nprojects Z∗to Z by\nZ = P Z∗.\n(5)\nWe assume that there is a Moore-Penrose-Inverse Z+ of Z∗\nwith Z∗Z+ = I, where I is the identity matrix. This states\ntrue if there are more data points than latent dimensions,\nwhich is always the case in a meaningful experimental set-\nting. The singular value decomposition of P ∗= ZZ+\ngives\nP ∗= USV T .\n(6)\nA new matrix S∗is deﬁned by copying S and setting all\nnonzero singular values to 1. Then the ideal rotation R can\nbe found by\nR = US∗V T .\n(7)\nThe desired latent positions are calculated by\n˜Z = RZ∗.\n(8)\nFigure 3. Visualization of the iteration steps. With each iteration the two classes are separated better in the latent space. The images show\nthe same two dimensions in every step for the 3D body shape dataset.\nSAE\n(ours)\nAE\nVAE\nMNIST\nHumanPose\nFashion-\nMNIST\nAAE\nFigure 4. The scatterplots show 2D projections of the latent space\nwhen using different types of autoencoders. For each instance an\nappropriate projection was chosen. Points of the same color repre-\nsent samples from the same class. It can be clearly seen that only\na Structuring Autoencoder is able to separate the latent variables\nwell.\nWith these target locations the autoencoder is trained batch-\nwise for a complete epoch. After the epoch the steps in\nthis section are repeated until convergence. The data in the\nlatent space during the training steps is visualized in Fig. 3.\n3. Experiments\nWe show the performance of our algorithm in sev-\neral experiments using diverse datasets including images\nand vector data.\nThe evaluation is done on the bench-\nmark datasets MNIST [16], the recently published fashion\ndatasets Fashion-MNIST [",
    "chunk_order_index": 2,
    "full_doc_id": "doc-f06c2ff9664d461ea472bbf88f2a9a4f"
  },
  "chunk-01919964963f385f6da15bcda957ea71": {
    "tokens": 1200,
    "content": "the autoencoder is trained batch-\nwise for a complete epoch. After the epoch the steps in\nthis section are repeated until convergence. The data in the\nlatent space during the training steps is visualized in Fig. 3.\n3. Experiments\nWe show the performance of our algorithm in sev-\neral experiments using diverse datasets including images\nand vector data.\nThe evaluation is done on the bench-\nmark datasets MNIST [16], the recently published fashion\ndatasets Fashion-MNIST [31] and DeepFashion2 [8], and\nour own 3D body shape dataset created using SMPL [19]. It\nis important to note, that we focus on artiﬁcially set classes.\nThat means we try to ﬁnd clusters that are not evident or\nbarely visible in the original data, e.g. a season depending\nsummer\nwinter\nall-year\nupper body clothes\nothers\nFigure 5. Comparison of two projections of the latent space using\ndifferent decompositions of the data. Note that the distance of two\nsamples is highly inﬂuenced by the chosen decomposition so this\nsetting is a method to individually control data.\ndecomposition of Fashion-MNIST. Furthermore, we show\nthat the SAE generalizes very well if only a small subset of\nthe training data is used. Since we achieve a clear separa-\ntion of the deﬁned classes in the latent space after training\nwe can ﬁt an optimal hyperplane between the classes using\nSupport Vector Machines [28]. This allows for the deﬁni-\ntion of a classiﬁcation error considering the separation in the\nlatent space. We further use the term reconstruction error\nas the root-mean-square error (RMSE) between the input\nand output of the autoencoder. We only train on unaug-\nmented data in all our experiments. This allows for a fair\nperformance comparison between different classiﬁers even\nfor data where no augmentation is possible, e.g. the 3D\nbody shape data. We are aware of the fact, that state-of-the-\nart classiﬁcation performance cannot be completely reached\nwithout data augmentation. However, we want to empha-\nsize that the focus of the paper is on semantically structuring\nthe latent space of autoencoders and not on state-of-the-art\nclassiﬁcation results on benchmark datasets. Therefore, we\nuse standard fully connected and convolutional neural net-\nworks for all experiments and compare against comparable\nclassiﬁcation networks. This means the classiﬁcation net-\nwork uses the same architecture as the encoder of the SAE\nto be compared plus a fully connected output layer.\n3.1. Datasets and Neural Networks\nTo show an example on a well-known benchmark dataset\nwe randomly divide MNIST into three classes A\n=\n(0, 1, 9), B\n= (4, 6, 8), and C\n= (2, 3, 5, 7).\nAs a\nMNIST\nFashion-MNIST\n3D body shapes\nFigure 6. Test error for different sizes of the training set without using data augmentation. The SAE outperforms a comparable traditional\nneural network and an adversarial autoencoder on each of the datasets signiﬁcantly, especially if the number of labeled training samples is\nlow.\nmore realistic example we evaluate on the Fashion-MNIST\ndataset which was published in 2017 to have a benchmark\nwhich is a lot harder than the old original MNIST. It con-\nsists of a training set of 60,000 examples and a test set of\n10,000 examples of various fashion items divided into 10\nclasses. According to the authors these images reﬂect real\nworld challenges in computer vision better than the origi-\nnal MNIST dataset. We split Fashion-MNIST into the three\nclasses summer clothes (top, sandals, dress and shirt), win-\nter clothes (pullover, coat and ankle boot), and all-year\nfashion (sneaker, trousers, bag).\nFor both datasets, MNIST and Fashion-MNIST, a convo-\nlutional neural network is used for the encoder. It consists\nof three 3 × 3 convolutional layers (8, 16, 32 ﬁlters), ReLU\nactivation and pooling layers. The latent space has a dimen-\nsion of 10 for MNIST and 64 for Fashion-MNIST.\nWe used a subset of DeepFashion2 dataset where we\nonly considered skirts and shorts to show the behaviour in\nborderline cases. For the encoder we use the convolutional\npart of the original VGG implementation [24] and a latent\nspace size of 192. In all networks the decoder always mir-\nrors the encoder.\nTo show general applicability for different types of data\nwe create a 3D HumanPose dataset that consists of ran-\ndomly created human models with 3000 male an 3000 fe-\nmale meshes in various poses and body shapes using SMPL\n[19]. We only use the x, y, z coordinates of the 6890 ver-\ntices for training by stacking them in a vector. Since the\ndata points are in vectorial form we use a fully connected\nnetwork consisting of two dense layers 2048 and 256 neu-\nrons, respectively. The latent space has 30 dimensions. This\ncovers a variety of data from simple images (MNIST) and\nmore complicated image (Fashion-MNIST) to data in vec-\ntorial form (3D HumanPose) and different network archi-\ntectures. Note that our approach is ﬂexible such that an ar-\nbitrary network structure can be applied for the encoder and\ndecoder networks.\n3.2",
    "chunk_order_index": 3,
    "full_doc_id": "doc-f06c2ff9664d461ea472bbf88f2a9a4f"
  },
  "chunk-a054a3e37bc1fcd86cb32715447f79a1": {
    "tokens": 1200,
    "content": "layers 2048 and 256 neu-\nrons, respectively. The latent space has 30 dimensions. This\ncovers a variety of data from simple images (MNIST) and\nmore complicated image (Fashion-MNIST) to data in vec-\ntorial form (3D HumanPose) and different network archi-\ntectures. Note that our approach is ﬂexible such that an ar-\nbitrary network structure can be applied for the encoder and\ndecoder networks.\n3.2. Structure Analysis\nAs already mentioned, some structure cannot be detected\nby traditional autoencoders because it is hidden in the data.\nThis effect can be visualized easily by projecting into the\nlatent space. Fig. 4 compares 2D projections of standard\nautoencoders (AE), variational autoencoders (VAE) and our\nproposed Structuring Autoencoders (SAE) for all datasets.\nStandard autoencoders barely show any structure in the\nform of clusters, whereas a slight clustering of samples of\nthe same class can be observed when using variational au-\ntoencoders. However, the desired clear separation cannot be\nseen at all while our SAE provides a clean structured latent\nspace. These examples use a ﬁxed distance of 1 between\nclasses. However, the inter class distance can be freely de-\nﬁned. Additionally, also the decomposition of the data is\nfree of choice. For example Fashion-MNIST can be decom-\nposed in another way, e. g. differentiating between clothes\nworn at the upper body and other fashion items. Fig. 5 com-\npares the projections of the resulting latent space using this\ndecomposition alongside the previously used one (summer,\nwinter, all-year).\n3.3. Improved Classiﬁcation\nSince the autoencoder separates the data in the latent\nspace it is possible to train a simple linear classiﬁer on the\nlatent space. We show that a linear SVM trained on the\nlatent variables achieves a better accuracy compared to a\nneural network of similar structure as the encoder. Since\nthe SAE enforces a latent space that can be decoded over-\nﬁtting is prevented even if only a small amount of training\ndata is used. Fig. 6 shows the error on the test set with\ndifferent numbers of labeled samples compared to an ad-\nversarial autoencoder and a neural network solely trained\nfor classiﬁcation. For the training of the adversarial autoen-\ncoder we performed the semi-supervised method described\nin Section 2.3 of the corresponding paper [20] and applied\nSVM after training. It can be clearly seen that the SAE\noutperforms traditional classiﬁcation networks on MNIST,\nFigure 7. Examples from the DeepFashion2 dataset where the class\nmembership is visually hard to determine or features of the op-\nposite class occur. In contrast to traditional classiﬁers the SAE\nassigns meaningful low conﬁdence values to these samples.\nFigure 8. Relation between the prediction score and the actual pre-\ncision which is computed over samples from binned sets of pre-\ndiction scores. Contrary to the noisy plot of the standard classiﬁer,\nthe smooth SAE plot shows that there is a clear mapping between\nprediction scores and the actual precision. Thus it is evident that\nthe scores provided by our SAE are much more reliable for critical\ndecisions.\nFashion-MNIST, and 3D HumanPose, especially when us-\ning only a few samples.\nNote that all experiments are done without data augmen-\ntation. For comparison, when applying data augmentation\nto the training data we achieve classiﬁcation rates of 99.04%\non MNIST using only 6000 samples.\n3.4. Decision Conﬁdence\nTraditional neural networks used for classiﬁcation aim\nto predict a class with high conﬁdence mostly applying a\nsoftmax activation in the last layer. As a result their de-\ncision conﬁdences appear to be relatively high even if the\nactual decision is uncertain. Our SAE avoids the uncer-\ntain predictions and gives a meaningful and interpretable\nconﬁdence measurement.\nIn real-world applications, for\ninstance reﬂected by the DeepFashion2 [8] data set, there\nare several samples that are hard to assign to one class be-\nFigure 9. Histogram of prediction scores when using a standard\nclassiﬁer and our SAE. While the standard classiﬁer tends to pre-\ndict scores near 0 and 1, the SAE outputs are more uniformly dis-\ntributed over the interval to reﬂect the conﬁdence better.\nFigure 10. Comparison of the guided and unguided sampling ap-\nproach for the MNIST initially trained on 600 samples. In epoch\n200 the 100 most uncertain assigned data points according to the\nSAE were added.\nThe standard classiﬁer threshold is a CNN\nof comparable structure as the encoder network which is solely\ntrained for classiﬁcation.\ncause of occlusions or the presence of features from several\nclasses. Therefore, it is desirable to have expressive predic-\ntion scores.\nFor example in Figure 7 some images of the DeepFash-\nion2 dataset [8] are shown where it is hard to determine if\nthe picture shows a skirt or shorts, even for a human ob-\nserver. We compared the prediction scores and their ex-\npressiveness of the SAE and an equivalent traditional clas-\nsiﬁer for skirts and shorts. We normalized the prediction\nscores provided by the SVM by scaling the scores between\nthe class centers into the interval [0...1]. Fig. 8 shows the\nrelation between the prediction scores and the actual preci-\nsion. The noisy",
    "chunk_order_index": 4,
    "full_doc_id": "doc-f06c2ff9664d461ea472bbf88f2a9a4f"
  },
  "chunk-3ae2925ca0a8a4c4599df3dd5cbb8f3a": {
    "tokens": 1200,
    "content": "is hard to determine if\nthe picture shows a skirt or shorts, even for a human ob-\nserver. We compared the prediction scores and their ex-\npressiveness of the SAE and an equivalent traditional clas-\nsiﬁer for skirts and shorts. We normalized the prediction\nscores provided by the SVM by scaling the scores between\nthe class centers into the interval [0...1]. Fig. 8 shows the\nrelation between the prediction scores and the actual preci-\nsion. The noisy graph of the traditional classiﬁer shows that\nthe prediction score provides only a rough evidence about\nthe class membership probability. For example the real pre-\ncision of 0.4 can be reﬂected by a prediction score between\n0.25 and 0.65. In contrast the stable and monotonous rela-\ntion when using the SAE shows that its prediction scores re-\nﬂect the uncertainty much better. That means the conﬁdence\ngiven by the SAE is much more reliable and expressive. In\ncontrast softmax activations in combination with cross en-\nSMPL body shapes\nMNIST\nFashion-MNIST\nFigure 11. Inﬂuence of the balancing parameter γ on the autoencoder error and the classiﬁcation error. For MNIST and Fashion-MNIST\nonly 6000 labeled training samples (10% of the data) were used. The training set of 3D body shape dataset consists of 1000 body shapes.\ntropy loss let traditional classiﬁers tend to predict scores that\nare either close to 1 or 0 as seen in Figure 9. Conﬁdences\nbetween these extremes are mostly noisy with a low infor-\nmative value. Structuring Autoencoders do not suffer from\nthis drawback since they naturally achieve a smooth separa-\ntion of the classes and make use of the reconstruction loss\ngiven by both the labeled and unlabeled samples. Regard-\ning only classiﬁcation tasks the reconstruction loss can also\nbe interpreted as a regularization term for the structural loss\nfunction.\n3.5. Guided Labeling\nSince the SAE combined with an SVM provides a reli-\nable decision conﬁdence it can be used to efﬁciently dis-\ncover important samples in the test set. After projecting\ninto the latent space samples with a high uncertainty for a\nclass do not show any exceptionally high SVM classiﬁca-\ntion score compared to the rest of the classes. We iden-\ntify these critical samples by calculating the scores for each\nclass and compare the highest score to the second high-\nest score. A small difference indicates a high uncertainty.\nThe most important of these data points under this criterion\ncan then be labeled manually and included in the training\ndata. This guides the training process such that only a small\namount of data needs to be labeled. To achieve a realis-\ntic setting we did not delete the points from the test set but\ninstead deﬁne an unlabeled set of samples from the train-\ning set of the respective datasets. Note that misclassiﬁed\ndata points are not detected by this method. However, our\nexperiments show that the classiﬁcation performance sig-\nniﬁcantly improves on the unchanged test set which means\nformerly misclassiﬁed samples are now correctly classiﬁed.\nFig. 10 shows the performance of a SAE combined with an\nSVM classiﬁer initially trained with 600 samples for 200\nepochs on MNIST. In epoch 200 the 100 most important\ndata points from the unlabeled set are automatically de-\ntected and included in the training set. This results in a\ndecrease of the classiﬁcation error from approximately 4%\nto 3%. It is compared against a SAE trained with randomly\nsampled data to show that the better performance is a result\nof the intelligent choice of new samples and not of the in-\ncreased number of samples. Additionally, we show that our\nmethods outperforms a neural network of the same struc-\nture as the encoder part of the SAE which is solely trained\nfor classiﬁcation. Using the guided labeling approach the\ntime and cost for manual annotations can be signiﬁcantly\nreduced since only the most important samples (i.e. the\nsamples with the highest uncertainty) need to be labeled\nmanually.\n3.6. Effect of MDS\nAs stated earlier our modiﬁcation to a standard autoen-\ncoder training only has a minor inﬂuence on the autoen-\ncoders reconstruction. This inﬂuence is regulated by the\nparameter γ in Eq. 3, where γ = 0 means that the struc-\ntural loss is ignored during training, i.e. a traditional au-\ntoencoder is trained. Setting γ = 1 means only the struc-\ntural loss is considered. Fig. 11 shows the reconstruction\nerror and the classiﬁcation error on the three datasets with\ndifferent values for γ. Assuming that a low reconstruction\nerror and a low classiﬁcation error is desired we can es-\ntimate the best values for γ in Fig. 11 as 0.5 for MNIST\nand 0.75 for Fashion-MNIST. The best value for 3D Hu-\nmanPose lies around 0.0041. The reconstruction error does\nnot increase much when applying the structural loss. That\nmeans the reconstructions remain equally good for a wide\nrange of values for γ.\nHaving a closer look at the results in Fig. 11 for MNIST\nand Fashion-MNIST reveals a slight rise when γ gets close\nto 1 (i.e. the network is mostly optimized for classiﬁcation",
    "chunk_order_index": 5,
    "full_doc_id": "doc-f06c2ff9664d461ea472bbf88f2a9a4f"
  },
  "chunk-25e488b3f3a1bc8c8ba9a749034b171e": {
    "tokens": 1200,
    "content": ". The best value for 3D Hu-\nmanPose lies around 0.0041. The reconstruction error does\nnot increase much when applying the structural loss. That\nmeans the reconstructions remain equally good for a wide\nrange of values for γ.\nHaving a closer look at the results in Fig. 11 for MNIST\nand Fashion-MNIST reveals a slight rise when γ gets close\nto 1 (i.e. the network is mostly optimized for classiﬁcation).\nThis underlines our claim that the SAE efﬁciently com-\nbines the natural structuring properties of traditional au-\ntoencoders with an additional structural information.\nFor subjective evaluation Fig. 12 shows some exam-\nple reconstructions for MNIST and Fashion-MNIST while\nFig. 13 shows examples for 3D HumanPose. The recon-\nstructions of the SAE and the traditional autoencoder are\nnearly indistinguishable.\n1This low weight can be explained by the numerical low reconstruction\nerror as seen in Fig. 11.\nFigure 12. Reconstructions of MNIST and Fashion-MNIST ob-\ntained by the SAE compared to ground truth and a standard au-\ntoencoder. The SAE produces a quality of output images that is\ncomparable and in some cases subjectively better compared to the\ntraditional autoencoder.\nFigure 13. Reconstructions (green) obtained by the SAE of the 3D\nbody shapes compared to ground truth (red). Body shape and pose\nare reconstructed well. Only minor deviations can be seen in the\nextremities.\n3.7. Class Transitions\nBy exploiting the separated latent space it is possible to\ntransition from one class to another. For visualization we\nuse the 3D HumanPose dataset and the corresponding au-\ntoencoder trained to separate into male and female body\nshapes. The deformation vector is deﬁned by the vector\nfrom the class center of the female class to the center of\nthe male class or vice-versa. To morph between classes the\nscaled deformation vector is added to the latent variables.\nThe morphed reconstruction is then obtained by applying\nthe decoder to the changed latent variables. The step-wise\nmorphing from male to female is visualized in Fig. 14. As\ncan be seen there is a smooth transition between the classes.\nInterestingly the body pose does not change much while\nmorphing. That means the autoencoder learns to structure\nthe latent space for the pose component by itself. Moreover,\nthis structure seems to be similar for the male and female\nclusters in the latent space. This underlines our claim that\nthe self structuring properties of traditional autoencoders\ncan be efﬁciently combined with another given structure us-\ning the SAE.\nFigure 14. Visualization of the body shape morphing in the latent\nspace. The two classes male and female are well separated. When\nadding the directional vector deﬁned by vector between the cen-\nters of the male and female clusters the male body shape clearly\ntransitions to female while maintaining the body pose.\n4. Conclusion\nWe presented a method to improve traditional autoen-\ncoders such that they are able to structure the latent space\naccording to given labels. Our SAE is able to separate dif-\nferent classes in the latent space even if this separation is\nnot present in the data. By combining the traditional Mul-\ntidimensional Scaling technique with novel autoencoder ar-\nchitectures the latent space is not only well structured but\nalso preserves predeﬁned distances between the different\nclasses. We showed that a simple linear classiﬁer on the\nlatent variables outperforms comparable neural networks in\nclassiﬁcation tasks. In sparsely-supervised settings the SAE\nhelps lowering the amount of required training data to re-\nduce labeling cost and time. At the same time the predic-\ntion of unknown samples is more interpretable which, un-\nlike standard classiﬁers, enables a reliable decision conﬁ-\ndence. Based on this we developed a guided labeling ap-\nproach by exploiting distances to class boundaries in the\nlatent space which detects the unlabeled data points with\nthe highest classiﬁcation uncertainty. Additionally, an ex-\nample for the combination of the self structuring properties\nof traditional autoencoders with the proposed MDS method\nis shown.\nOur proposed SAE could be used in the fu-\nture to improve tasks like human pose estimation [30] and\nanomaly detection [32]. Furthermore, it may be combined\nwith Markov Chain Neural Networks [2].\nReferences\n[1] M. Awiszus, H. Ackermann, and B. Rosenhahn. Learning\ndisentangled representations via independent subspaces. In\nThird International Workshop on ”Robust Subspace Learn-\ning and Applications in Computer Vision”, 2019.\n[2] M. Awiszus and B. Rosenhahn. Markov chain neural net-\nworks. In Computer Vision and Pattern Recognition Work-\nshops (CVPRW), June 2018.\n[3] H. Bourlard and Y. Kamp. Auto-association by multilayer\nperceptrons and singular value decomposition.\nBiological\nCybernetics, 59(4):291–294, Sep 1988.\n[4] M. A. Carreira-Perpi˜n´an and R. Raziperchikolaei.\nHash-\ning with binary autoencoders.\nIn 2015 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n557–566, June 2015.\n[5] M. Chen, Z. Xu, K. Weinberger, and F. Sha. Marginalized\ndenoising autoencoders for domain adaptation. In J. Lang-\nford",
    "chunk_order_index": 6,
    "full_doc_id": "doc-f06c2ff9664d461ea472bbf88f2a9a4f"
  },
  "chunk-7f621f55aaa10556978c3ae269d56c60": {
    "tokens": 1200,
    "content": ". A. Carreira-Perpi˜n´an and R. Raziperchikolaei.\nHash-\ning with binary autoencoders.\nIn 2015 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n557–566, June 2015.\n[5] M. Chen, Z. Xu, K. Weinberger, and F. Sha. Marginalized\ndenoising autoencoders for domain adaptation. In J. Lang-\nford and J. Pineau, editors, Proceedings of the 29th Interna-\ntional Conference on Machine Learning (ICML-12), ICML\n’12, pages 767–774. ACM, New York, NY, USA, July 2012.\n[6] Y. Chen, L. Zhang, and Z. Yi. Subspace clustering using\na low-rank constrained autoencoder. Information Sciences,\n424:27–38, 2018.\n[7] C. Donahue, A. Balsubramani, J. McAuley, and Z. C. Lip-\nton. Semantically decomposing the latent spaces of gener-\native adversarial networks. In International Conference on\nLearning Representations, 2018.\n[8] Y. Ge, R. Zhang, L. Wu, X. Wang, X. Tang, and P. Luo.\nDeepfashion2: A versatile benchmark for detection, pose es-\ntimation, segmentation and re-identiﬁcation of clothing im-\nages. CoRR, abs/1901.07973, 2019.\n[9] S. Graßhof, H. Ackermann, S. S. Brandt, and J. Ostermann.\nApathy is the root of all expressions. In 2017 12th IEEE In-\nternational Conference on Automatic Face & Gesture Recog-\nnition (FG 2017), pages 658–665. IEEE, 2017.\n[10] S. Gu, J. Bao, H. Yang, D. Chen, F. Wen, and L. Yuan. Mask-\nguided portrait editing with conditional gans. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3436–3445, 2019.\n[11] G. E. Hinton and R. R. Salakhutdinov.\nReducing the\ndimensionality of data with neural networks.\nscience,\n313(5786):504–507, 2006.\n[12] D. P. Kingma and M. Welling. Auto-encoding variational\nbayes. CoRR, abs/1312.6114, 2013.\n[13] J. B. Kruskal.\nMultidimensional scaling by optimizing\ngoodness of ﬁt to a nonmetric hypothesis. Psychometrika,\n29(1):1–27, Mar 1964.\n[14] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum.\nDeep convolutional inverse graphics network. In C. Cortes,\nN. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,\neditors, Advances in Neural Information Processing Systems\n28, pages 2539–2547. Curran Associates, Inc., 2015.\n[15] Y. LeCun. Generalization and network design strategies. In\nR. Pfeifer, Z. Schreter, F. Fogelman, and L. Steels, editors,\nConnectionism in Perspective, Zurich, Switzerland, 1989.\nElsevier. an extended version was published as a technical\nreport of the University of Toronto.\n[16] Y. LeCun and C. Cortes. MNIST handwritten digit database.\n2010.\n[17] F. Li, H. Qiao, and B. Zhang. Discriminatively boosted im-\nage clustering with fully convolutional auto-encoders. Pat-\ntern Recognition, 83:161 – 173, 2018.\n[18] H. Liu, M. Shao, S. Li, and Y. Fu. Inﬁnite ensemble for\nimage clustering. In Proceedings of the 22Nd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data\nMining, KDD ’16, pages 1745–1754, New York, NY, USA,\n2016. ACM.\n[19] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J.\nBlack. SMPL: A skinned multi-person linear model. ACM\nTrans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1–\n248:16, Oct. 2015.\n[20] A. Makhzani, J. Shlens, N. Jaitly, and I. Goodfellow. Adver-\nsarial autoencoders. In International Conference on Learn-\ning Representations, 2016.\n[21] L. McInnes and J. Healy. UMAP: Uniform Manifold Ap-\nproximation and Projection for Dimension Reduction. ArXiv\ne-prints, Feb. 2018.\n[22] A. Roberts, J. Engel, C. Raffel, C. Hawthorne, and D. Eck. A\nhierarchical latent vector model for learning long-term struc-\nture in music. In J. Dy and A. Krause, editors, Proceedings\nof the 35th International Conference on Machine Learning,\nvolume 80 of Proceedings of Machine Learning Research,\npages 4364–4373, Stockholmsm¨assan, Stockholm Sweden,\n10–15 Jul 2018.",
    "chunk_order_index": 7,
    "full_doc_id": "doc-f06c2ff9664d461ea472bbf88f2a9a4f"
  },
  "chunk-2d7c1c465da6845a22d14f73db6b298c": {
    "tokens": 706,
    "content": ", J. Engel, C. Raffel, C. Hawthorne, and D. Eck. A\nhierarchical latent vector model for learning long-term struc-\nture in music. In J. Dy and A. Krause, editors, Proceedings\nof the 35th International Conference on Machine Learning,\nvolume 80 of Proceedings of Machine Learning Research,\npages 4364–4373, Stockholmsm¨assan, Stockholm Sweden,\n10–15 Jul 2018. PMLR.\n[23] P. H. Sch¨onemann. A generalized solution of the orthogonal\nprocrustes problem. Psychometrika, 31(1):1–10, Mar 1966.\n[24] K. Simonyan and A. Zisserman.\nVery deep convolu-\ntional networks for large-scale image recognition.\nCoRR,\nabs/1409.1556, 2014.\n[25] C. Song, F. Liu, Y. Huang, L. Wang, and T. Tan.\nAuto-\nencoder based data clustering.\nIn J. Ruiz-Shulcloper and\nG. Sanniti di Baja, editors, Progress in Pattern Recognition,\nImage Analysis, Computer Vision, and Applications, pages\n117–124, Berlin, Heidelberg, 2013. Springer Berlin Heidel-\nberg.\n[26] L. van der Maaten. Accelerating t-sne using tree-based al-\ngorithms. Journal of Machine Learning Research, 15:3221–\n3245, 2014.\n[27] L. van der Maaten and G. Hinton.\nVisualizing high-\ndimensional data using t-sne. Journal of Machine Learning\nResearch, 9:2579–2605, 2008.\n[28] V. N. Vapnik and A. Y. Chervonenkis.\nTheory of Pattern\nRecognition. Nauka, USSR, 1974.\n[29] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A.\nManzagol. Stacked denoising autoencoders: Learning use-\nful representations in a deep network with a local denoising\ncriterion. J. Mach. Learn. Res., 11:3371–3408, Dec. 2010.\n[30] B. Wandt and B. Rosenhahn. Repnet: Weakly supervised\ntraining of an adversarial reprojection network for 3d human\npose estimation. In The IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), June 2019.\n[31] H. Xiao, K. Rasul, and R. Vollgraf.\nFashion-mnist:\na\nnovel image dataset for benchmarking machine learning al-\ngorithms. arXiv preprint arXiv:1708.07747, 2017.\n[32] M. Y. Yang, W. Liao, Y. Cao, and B. Rosenhahn. Video event\nrecognition and anomaly detection by combining gaussian\nprocess and hierarchical dirichlet process models. In Pho-\ntogrammetric Engineering & Remote Sensing, 2018.\n[33] F. W. Young. Multidimensional Scaling: History, Theory,\nand Applications. Lawrence, Erlbaum Associates, Publish-\ners (Hillsdale, New Jersey; London), 1987.",
    "chunk_order_index": 8,
    "full_doc_id": "doc-f06c2ff9664d461ea472bbf88f2a9a4f"
  }
}